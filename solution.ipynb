{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6324b330-9daa-495f-a4a5-17989f4839f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f56dfb0-aba5-42e4-83ed-e7f8d225c237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>PL</th>\n",
       "      <th>VaRBHS</th>\n",
       "      <th>VaREWMA</th>\n",
       "      <th>VaRn</th>\n",
       "      <th>VaRt</th>\n",
       "      <th>VaRPot</th>\n",
       "      <th>ESEWMA</th>\n",
       "      <th>ESn</th>\n",
       "      <th>ESt</th>\n",
       "      <th>ESPot</th>\n",
       "      <th>Losses</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-12-26</td>\n",
       "      <td>-150</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1147.464693</td>\n",
       "      <td>777.535296</td>\n",
       "      <td>986.872911</td>\n",
       "      <td>1391.726372</td>\n",
       "      <td>2544.116901</td>\n",
       "      <td>897.215494</td>\n",
       "      <td>3209.017239</td>\n",
       "      <td>1924.333523</td>\n",
       "      <td>150</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-12-28</td>\n",
       "      <td>150</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1144.195603</td>\n",
       "      <td>778.321836</td>\n",
       "      <td>987.104816</td>\n",
       "      <td>1391.726372</td>\n",
       "      <td>2540.137912</td>\n",
       "      <td>898.017553</td>\n",
       "      <td>3189.914672</td>\n",
       "      <td>1924.333523</td>\n",
       "      <td>-150</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-12-29</td>\n",
       "      <td>-20</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1117.460475</td>\n",
       "      <td>778.334975</td>\n",
       "      <td>987.885905</td>\n",
       "      <td>1391.726372</td>\n",
       "      <td>2506.631070</td>\n",
       "      <td>898.020953</td>\n",
       "      <td>3203.255498</td>\n",
       "      <td>1924.333523</td>\n",
       "      <td>20</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-12-31</td>\n",
       "      <td>30</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1085.901247</td>\n",
       "      <td>777.503184</td>\n",
       "      <td>989.976454</td>\n",
       "      <td>1391.726372</td>\n",
       "      <td>2468.406961</td>\n",
       "      <td>897.126264</td>\n",
       "      <td>3261.771712</td>\n",
       "      <td>1924.333523</td>\n",
       "      <td>-30</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-01-01</td>\n",
       "      <td>30</td>\n",
       "      <td>1115.0</td>\n",
       "      <td>1053.756229</td>\n",
       "      <td>777.645897</td>\n",
       "      <td>990.797723</td>\n",
       "      <td>1391.726372</td>\n",
       "      <td>2431.278879</td>\n",
       "      <td>897.263546</td>\n",
       "      <td>3270.873551</td>\n",
       "      <td>1924.333523</td>\n",
       "      <td>-30</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date   PL  VaRBHS      VaREWMA        VaRn        VaRt       VaRPot  \\\n",
       "0 2006-12-26 -150  1115.0  1147.464693  777.535296  986.872911  1391.726372   \n",
       "1 2006-12-28  150  1115.0  1144.195603  778.321836  987.104816  1391.726372   \n",
       "2 2006-12-29  -20  1115.0  1117.460475  778.334975  987.885905  1391.726372   \n",
       "3 2006-12-31   30  1115.0  1085.901247  777.503184  989.976454  1391.726372   \n",
       "4 2007-01-01   30  1115.0  1053.756229  777.645897  990.797723  1391.726372   \n",
       "\n",
       "        ESEWMA         ESn          ESt        ESPot  Losses  Year  \n",
       "0  2544.116901  897.215494  3209.017239  1924.333523     150  2006  \n",
       "1  2540.137912  898.017553  3189.914672  1924.333523    -150  2006  \n",
       "2  2506.631070  898.020953  3203.255498  1924.333523      20  2006  \n",
       "3  2468.406961  897.126264  3261.771712  1924.333523     -30  2006  \n",
       "4  2431.278879  897.263546  3270.873551  1924.333523     -30  2007  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Excel file\n",
    "file_path = \"DataLab2.xlsx\" \n",
    "\n",
    "# Read the first 501 to 1009 rows and first 11 columns\n",
    "df = pd.read_excel(file_path, header=0, skiprows=range(1, 501), usecols=range(11))\n",
    "\n",
    "# Transform 'PL' column to create 'Losses' column\n",
    "df[\"Losses\"] = df[\"PL\"] * (-1)\n",
    "\n",
    "# extract the years\n",
    "df[\"Year\"] = df[\"Date\"].dt.year.astype(int)\n",
    "\n",
    "# Display the first few rows (for testing)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fcd18e4-e2bf-4ff4-be15-91f0e32a809b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Backtesting VAR\n",
    "\n",
    "# encode exceptions for each VAR\n",
    "var_indices = list(range(2,7)) # these are the columns indices for the VAR\n",
    "\n",
    "# to get all the names of the headers for the VAR so we can use these when encoding\n",
    "var_names = df.columns[var_indices] \n",
    "\n",
    "# here we create the headers for the exceptions, 0 no exception, 1 is.\n",
    "ex_names = [f'{var_name}_exception' for var_name in var_names] \n",
    "\n",
    "# we zip the lists just to be able to loop over both\n",
    "var_ex_names = list(zip(var_names, ex_names)) \n",
    "\n",
    "# here we encode a violation (exception) as a 1, otherwise 0\n",
    "for var_name, ex_name in var_ex_names:\n",
    "    df[ex_name] = (df[var_name] < df[\"Losses\"]).astype(int) \n",
    "\n",
    "# Get indices of the exception columns (for easy retrieval later)\n",
    "ex_indices = [df.columns.get_loc(ex_name) for ex_name in ex_names]\n",
    "\n",
    "# the indices of the ES\n",
    "es_indices = list(range(7,11))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d728d41-dac2-42e4-ab0f-c271528ad3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     VaRBHS      VaREWMA         VaRn         VaRt       VaRPot       ESEWMA  \\\n",
      "4    1115.0  1053.756229   777.645897   990.797723  1391.726372  2431.278879   \n",
      "5    1115.0  1021.701152   777.393080   993.867845  1391.726372  2391.877505   \n",
      "6    1115.0   990.624063   777.083717   994.978159  1391.726372  2355.183490   \n",
      "7    1115.0   966.503238   777.332311   993.984109  1391.726372  2326.891025   \n",
      "8    1115.0   943.122243   775.856105   996.059063  1391.726372  2300.318478   \n",
      "..      ...          ...          ...          ...          ...          ...   \n",
      "251  2720.0  5446.739118  1850.330910  1872.782072  3923.705072  8667.750829   \n",
      "252  2720.0  5281.329790  1851.518976  1880.739916  3923.705072  8417.587184   \n",
      "253  2720.0  5140.982010  1854.690014  1898.367178  3923.705072  8194.339146   \n",
      "254  2720.0  5025.876542  1891.472008  1899.165832  3923.705072  7982.628781   \n",
      "255  2720.0  5636.548968  1892.561404  1911.666403  3923.705072  8916.984522   \n",
      "\n",
      "             ESn           ESt        ESPot  Losses  VaRBHS_exception  \\\n",
      "4     897.263546   3270.873551  1924.333523     -30                 0   \n",
      "5     896.851545   3349.147061  1924.333523    -200                 0   \n",
      "6     896.435939   3393.172268  1924.333523     110                 0   \n",
      "7     896.700352   3367.117193  1924.333523     -50                 0   \n",
      "8     895.111080   3449.972173  1924.333523    -160                 0   \n",
      "..           ...           ...          ...     ...               ...   \n",
      "251  2105.484299  16767.872162  4965.546229     550                 0   \n",
      "252  2106.755112  16837.430181  4965.546229     790                 0   \n",
      "253  2110.102556  16978.790682  4965.546229   -3710                 0   \n",
      "254  2153.331951  16991.322099  4965.546229    -840                 0   \n",
      "255  2154.760658  17099.514017  4965.546229    -650                 0   \n",
      "\n",
      "     VaREWMA_exception  VaRn_exception  VaRt_exception  VaRPot_exception  \n",
      "4                    0               0               0                 0  \n",
      "5                    0               0               0                 0  \n",
      "6                    0               0               0                 0  \n",
      "7                    0               0               0                 0  \n",
      "8                    0               0               0                 0  \n",
      "..                 ...             ...             ...               ...  \n",
      "251                  0               0               0                 0  \n",
      "252                  0               0               0                 0  \n",
      "253                  0               0               0                 0  \n",
      "254                  0               0               0                 0  \n",
      "255                  0               0               0                 0  \n",
      "\n",
      "[252 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# now to actually doing the tests for each year, alpha=0.99 was used in the first exercise for the vars, so we use that here\n",
    "# first create a python dictionary for all the years, so we extract each row (with relevant data) for each year\n",
    "# I also add the ES estimates since this will be useful later\n",
    "\n",
    "years_data = {}\n",
    "loss_index = [df.columns.get_loc(\"Losses\")]\n",
    "\n",
    "# here we create all the necessary indices\n",
    "var_ex_es_loss_indices = var_indices + es_indices +loss_index+ ex_indices\n",
    "\n",
    "\n",
    "for row, year in enumerate(df[\"Year\"]):\n",
    "    if year == 2006: # we skip year 2006 since we don't have enough data there\n",
    "        continue\n",
    "\n",
    "    # access all the data\n",
    "    data = df.iloc[row, var_ex_es_loss_indices]\n",
    "    \n",
    "    if year in years_data:\n",
    "        years_data[year].append(data)\n",
    "    else:\n",
    "        years_data[year] = [data]\n",
    "\n",
    "# convert each year into a separate dataframe for easy analysis\n",
    "for year in years_data:\n",
    "    years_data[year] = pd.DataFrame(years_data[year])\n",
    "\n",
    "print(years_data[2007])\n",
    "# now we have a python dict with each year with the corresponding losses for that year, and the VARs for those years, \n",
    "#exceptions in order aswell inside each year, and ES for each year and day of that year after 2006 since we append."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66df0619-f9cc-4bc0-a6c9-25c34d279f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will actually be doing the tests on each of the VARs\n",
    "# since we only really care about underestimating losses, we will do one sided tests\n",
    "\n",
    "#will be using the binomial distribution from scipy\n",
    "from scipy.stats import binom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8994470-7190-44ab-92ee-a8317cf40e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function for the kupiec test\n",
    "def kupiec(ex_list: list, alpha=0.99, significance_level=0.05) -> str:\n",
    "    # Total number of observations (days)\n",
    "    n = len(ex_list)\n",
    "    \n",
    "    # Number of violations (exceptions should be 1 for a violation, 0 otherwise)\n",
    "    k = sum(ex_list)\n",
    "    \n",
    "    # The expected probability of a violation under the null hypothesis\n",
    "    p = 1 - alpha\n",
    "\n",
    "    # Handle the case with 0 violations explicitly for clarity\n",
    "    if k == 0:\n",
    "        p_value = 1.0\n",
    "    else:\n",
    "        # For k >= 1, compute the p-value as the probability of seeing at least k violations\n",
    "        p_value = 1 - binom.cdf(k - 1, n, p)\n",
    "    \n",
    "    # Return True if we reject the null hypothesis (p-value is less than the significance level)\n",
    "    test = p_value < significance_level \n",
    "    if test:\n",
    "        return \"Reject\"\n",
    "    else:\n",
    "        return \"Accept\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a85011d-bc3d-4195-8af5-dace5662c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# function for basel\n",
    "def basel(ex_list: list, alpha = 0.99) -> str:\n",
    "    n = len(ex_list)\n",
    "    p = 1-alpha\n",
    "    k = sum(ex_list)  # Count exceptions\n",
    "\n",
    "    # Compute the cutoffs dynamically\n",
    "    green_yellow = int(binom.ppf(0.95, n, p))  # 95% cutoff\n",
    "    yellow_red = int(binom.ppf(0.99, n, p))  # 99% cutoff\n",
    "\n",
    "    # Assign traffic light zone\n",
    "    if k <= green_yellow:\n",
    "        return \"Green\"\n",
    "    elif green_yellow < k <= yellow_red:\n",
    "        return \"Yellow\"\n",
    "    else:\n",
    "        return \"Red\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd9b2033-674a-489b-b889-25c53f7f06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for christofferson\n",
    "from scipy.stats import chi2\n",
    "def christofferson(ex_list: list, significance_level = 0.05) -> str:\n",
    "    ex_list = list(ex_list)\n",
    "    \n",
    "    # initialize transition counts\n",
    "    n_00 = n_01 = n_10 = n_11 = 0\n",
    "\n",
    "    for i in range(1, len(ex_list)):  # Start at index 1 to check previous day\n",
    "        prev, curr = ex_list[i - 1], ex_list[i]\n",
    "        if prev == 0 and curr == 0:\n",
    "            n_00 += 1\n",
    "        elif prev == 0 and curr == 1:\n",
    "            n_01 += 1\n",
    "        elif prev == 1 and curr == 0:\n",
    "            n_10 += 1\n",
    "        elif prev == 1 and curr == 1:\n",
    "            n_11 += 1\n",
    "\n",
    "    # Calculate transition probabilities\n",
    "    pi_01 = n_01 / (n_00 + n_01) if (n_00 + n_01) > 0 else 0  \n",
    "    pi_00 = 1 - pi_01 # the complement\n",
    "    \n",
    "    pi_11 = n_11 / (n_10 + n_11) if (n_10 + n_11) > 0 else 0\n",
    "    pi_10 = 1 - pi_11\n",
    "\n",
    "    # compute L1\n",
    "    L1 = pi_00**n_00 * pi_01**n_01 * pi_10**n_10 * pi_11**n_11 # chat says this might cause underflow errors but I think it is fine\n",
    "    \n",
    "    \n",
    "    n_1 = sum(ex_list) # number of violations, look at the video\n",
    "    n = len(ex_list) # length of list\n",
    "    n_0 = n - n_1 # number of zeros\n",
    "    pi_0 = n_1/n # expected probability\n",
    "    pi_1 = n_1/n # or equivalently, just the complement\n",
    "\n",
    "    #compute L0\n",
    "    L0 = pi_0**n_0 * pi_1**n_1\n",
    "\n",
    "    # compute the test\n",
    "    LR = -2*(np.log(L0) - np.log(L1))if L1 > 0 and L0 > 0 else np.inf\n",
    "    \n",
    "    chi2_critical = chi2.ppf(1-significance_level, df=1)\n",
    "    test_rule = LR > chi2_critical\n",
    "    \n",
    "    if test_rule:\n",
    "        return \"Reject\"\n",
    "    else:\n",
    "        return \"Accept\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2cb7e91c-7d6f-458b-9f11-b9f6ebbeb228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will be performing each of the tests\n",
    "\n",
    "# create a dictionary keeping track of the test results, for each year, on each var (very complicated structure but works wonders)\n",
    "test_data_var = {\n",
    "    year: \n",
    "               {\n",
    "        var_name:\n",
    "                    {\n",
    "    \"Kupiec\":None, \n",
    "    \"Basel\":None,\n",
    "    \"Christofferson\":None\n",
    "                    }\n",
    "    for var_name in var_names\n",
    "               }\n",
    "    for year in years_data\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7c6e187-7696-4c82-a1f4-e159e800858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all the years, extract the exception columns, and perform each test on each encoded exception\n",
    "for year, year_dataframe in years_data.items():\n",
    "\n",
    "    #this gives all the exception columns for that year\n",
    "    exceptions_columns = year_dataframe[ex_names] \n",
    "\n",
    "    #var_ex_names is a zipped list with the var and corresponding exceptions column name\n",
    "    for var_name, ex_name in var_ex_names: \n",
    "        \n",
    "        # the violations for that current type of var for that year\n",
    "        ex_list = exceptions_columns[ex_name] \n",
    "\n",
    "         # here we do the kupiec test and store everything for that test, that var, that year\n",
    "        test_data_var[year][var_name][\"Kupiec\"] = kupiec(ex_list)\n",
    "\n",
    "        # here we do the basel test\n",
    "        test_data_var[year][var_name][\"Basel\"] = basel(ex_list)\n",
    "\n",
    "        # here we do the christofferson test\n",
    "        test_data_var[year][var_name][\"Christofferson\"] = christofferson(ex_list)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26a09c17-c1d5-4eea-8588-dacccbb0ba2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data for 2007:\n",
      "         Kupiec  Basel Christofferson\n",
      "VaRBHS   Reject    Red         Reject\n",
      "VaREWMA  Accept  Green         Reject\n",
      "VaRn     Reject    Red         Reject\n",
      "VaRt     Reject    Red         Reject\n",
      "VaRPot   Accept  Green         Reject\n",
      "\n",
      "Data for 2008:\n",
      "         Kupiec   Basel Christofferson\n",
      "VaRBHS   Reject  Yellow         Reject\n",
      "VaREWMA  Reject  Yellow         Reject\n",
      "VaRn     Reject     Red         Reject\n",
      "VaRt     Reject     Red         Reject\n",
      "VaRPot   Accept   Green         Reject\n"
     ]
    }
   ],
   "source": [
    "# convert the data for nice visualization\n",
    "year_with_dfs_var = {year: pd.DataFrame.from_dict(data, orient='index') for year, data in test_data_var.items()}\n",
    "\n",
    "# Display each DataFrame\n",
    "for year, year_df in year_with_dfs_var.items():\n",
    "    print(f\"\\nData for {year}:\")\n",
    "    print(year_df)\n",
    "\n",
    "# Accept Kupiec means VaR estimate is good\n",
    "# Basel are the different zones\n",
    "# Christofferson, Rejecting means States dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d355fed8-e424-461f-8158-eaeaa2f41444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting ES \n",
    "# Now we are moving on to ES estimates, we will in essence build the same kind of dataframe structure but do it for ES instead\n",
    "\n",
    "test_data_es = {year: {es_name:None for es_name in es_names} for year in years_data}\n",
    "\n",
    "#the names of the columns for ES\n",
    "es_names = df.columns[es_indices]\n",
    "\n",
    "# list with var and ES for easy retrieval later, there doesn't seem to be any data for ESBHS so I guess we just skip that. \n",
    "ex_es_names = list(zip(ex_names[1:],es_names)) # if that data is added I can fix it later by removing the 1 and fixing the columns indices above\n",
    "\n",
    "#redefining alpha for readability\n",
    "alpha = 0.99\n",
    "\n",
    "# here we define the Acerbi-Szekely\n",
    "def acerbi_szekely(L,ES,I):\n",
    "    M = len(L)\n",
    "    Z = - 1/(M*(1-alpha)) * sum(L*I / ES) + 1\n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2accbc3-d6b4-4a0b-b575-95aa069b308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2007: {'ESEWMA': -0.4660637500834959, 'ESn': -18.46196086363861, 'ESt': -1.3059063982648045, 'ESPot': -1.1669705966696289}, 2008: {'ESEWMA': -1.0486784699824647, 'ESn': -6.577422947232897, 'ESt': 0.11051484562715497, 'ESPot': -0.502133569043556}}\n"
     ]
    }
   ],
   "source": [
    "# loop over all the years, extract the indicator columns, and perform each test on each encoded exception\n",
    "\n",
    "for year, year_dataframe in years_data.items():\n",
    "    # here we extract the losses for that year\n",
    "    losses = year_dataframe[\"Losses\"]\n",
    "    \n",
    "    #this gives all the indicator columns for that year (exceptions essentially), still needed for ES since this is indicator\n",
    "    indicator_columns = year_dataframe[ex_names] \n",
    "\n",
    "    # here we extract the es columns\n",
    "    es_columns = year_dataframe[es_names]\n",
    "\n",
    "    #var_ex_es_names is a zipped list with the var, corresponding exceptions, es column name\n",
    "    for ex_name, es_name in ex_es_names: \n",
    "        \n",
    "        # the violations for that current type of var for that year\n",
    "        indicator = indicator_columns[ex_name] \n",
    "\n",
    "        es = es_columns[es_name]\n",
    "\n",
    "         # here we do the kupiec test and store everything for that test, that var, that year\n",
    "        test_data_es[year][es_name] = acerbi_szekely(losses,es,indicator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85c1404a-e530-4fdd-8836-df3cbcd00e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected Shortfall Data for 2007:\n",
      "                0\n",
      "ESEWMA  -0.466064\n",
      "ESn    -18.461961\n",
      "ESt     -1.305906\n",
      "ESPot   -1.166971\n",
      "\n",
      "Expected Shortfall Data for 2008:\n",
      "               0\n",
      "ESEWMA -1.048678\n",
      "ESn    -6.577423\n",
      "ESt     0.110515\n",
      "ESPot  -0.502134\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert test_data_es into a dictionary of DataFrames\n",
    "year_with_dfs_es = {year: pd.DataFrame.from_dict(data, orient='index') for year, data in test_data_es.items()}\n",
    "\n",
    "# Display each DataFrame\n",
    "for year, year_df in year_with_dfs_es.items():\n",
    "    print(f\"\\nExpected Shortfall Data for {year}:\")\n",
    "    print(year_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c8000a-6e44-45d9-8058-f0e02076d7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
